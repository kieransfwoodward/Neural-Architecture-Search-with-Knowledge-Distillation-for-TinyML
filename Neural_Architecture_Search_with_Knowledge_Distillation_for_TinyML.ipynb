{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6980d579",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6980d579"
      },
      "source": [
        "# Neural Architecture Search with Knowledge Distillation for TinyML\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a Neural Architecture Search (NAS) framework combined with Knowledge Distillation, aimed at finding efficient model architectures for TinyML applications. The primary goal is to discover compact, high-performing neural network architectures that can be deployed on resource-constrained devices.\n",
        "\n",
        "Key components and techniques:\n",
        "\n",
        "1. **Neural Architecture Search (NAS)**: Systematically explores various model architectures, focusing on configurations suitable for small devices.\n",
        "\n",
        "2. **Knowledge Distillation**: Utilises a larger, pre-trained teacher model to guide the training of smaller student models, potentially improving their performance.\n",
        "\n",
        "3. **Model Pruning**: Applies network pruning to reduce model size and potentially improve generalization.\n",
        "\n",
        "4. **Quantization**: Implements quantization to further reduce model size and improve inference speed.\n",
        "\n",
        "5. **CIFAR-10 Dataset**: Uses the CIFAR-10 dataset (converted to grayscale) as a benchmark for evaluating model performance.\n",
        "\n",
        "6. **Efficiency Metrics**: Considers both model accuracy and size to identify the best architectures for TinyML applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81751110",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "81751110"
      },
      "source": [
        "# Setup and Imports\n",
        "\n",
        "This cell imports the necessary libraries and modules for our neural architecture search (NAS) with knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7af71e",
      "metadata": {
        "id": "0d7af71e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import itertools\n",
        "import tempfile\n",
        "import os\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import deque\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3465dbd3",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "3465dbd3"
      },
      "source": [
        "# Distiller Class\n",
        "\n",
        "This class implements the knowledge distillation process. It combines the loss from the student model's predictions and the distillation loss from comparing the student's softened predictions to the teacher's softened predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90193a87",
      "metadata": {
        "id": "90193a87"
      },
      "outputs": [],
      "source": [
        "# Distiller class\n",
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3):\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        with tf.GradientTape() as tape:\n",
        "            student_predictions = self.student(x, training=True)\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1)\n",
        "            ) * (self.temperature ** 2)\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return self.student(inputs, training=training)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37397734",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "37397734"
      },
      "source": [
        "# Search Space Definition\n",
        "\n",
        "Here we define the possible layers and parameters that our neural architecture search will explore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d068f2",
      "metadata": {
        "id": "b0d068f2"
      },
      "outputs": [],
      "source": [
        "# # Define the search space\n",
        "# layer_types = ['Conv2D', 'Dense', 'Flatten', 'MaxPool2D', 'GlobalAveragePooling2D']\n",
        "# conv_filters = [16, 32, 64]\n",
        "# dense_units = [64, 128, 256]\n",
        "# max_layers = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc08f86",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6cc08f86"
      },
      "source": [
        "# Model Creation Function\n",
        "\n",
        "This function creates a Keras model based on a given configuration. It ensures that the model structure is valid, handling the transition from convolutional to dense layers appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9d08ba",
      "metadata": {
        "id": "5c9d08ba"
      },
      "outputs": [],
      "source": [
        "# Function to create a model given a configuration\n",
        "def create_model(config, input_shape, num_classes):\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.Input(shape=input_shape))\n",
        "\n",
        "    has_flattened = False\n",
        "    for layer in config[:-1]:  # Apply all layers except the last one\n",
        "        if layer[0] == 'Conv2D':\n",
        "            if has_flattened:\n",
        "                continue  # Skip Conv2D if we've already flattened\n",
        "            model.add(keras.layers.Conv2D(layer[1], (3, 3), activation='relu', padding='same'))\n",
        "        elif layer[0] == 'Dense':\n",
        "            if not has_flattened:\n",
        "                model.add(keras.layers.Flatten())\n",
        "                has_flattened = True\n",
        "            model.add(keras.layers.Dense(layer[1], activation='relu'))\n",
        "        elif layer[0] == 'Flatten':\n",
        "            if not has_flattened:\n",
        "                model.add(keras.layers.Flatten())\n",
        "                has_flattened = True\n",
        "        elif layer[0] == 'MaxPool2D':\n",
        "            if not has_flattened:\n",
        "                model.add(keras.layers.MaxPool2D((2, 2)))\n",
        "        elif layer[0] == 'GlobalAveragePooling2D':\n",
        "            if not has_flattened:\n",
        "                model.add(keras.layers.GlobalAveragePooling2D())\n",
        "                has_flattened = True\n",
        "\n",
        "    # Ensure the model is flattened before the final Dense layer\n",
        "    if not has_flattened:\n",
        "        model.add(keras.layers.Flatten())\n",
        "\n",
        "    # Always add the final Dense layer with softmax activation\n",
        "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca037b82",
      "metadata": {
        "id": "ca037b82"
      },
      "source": [
        "# Quick Evaluation Function\n",
        "\n",
        "This function performs a rapid evaluation of a model using a small number of epochs and early stopping. It's used to quickly filter out poorly performing architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba41d40",
      "metadata": {
        "id": "5ba41d40"
      },
      "outputs": [],
      "source": [
        "def quick_evaluate(model, x_train, y_train, x_val, y_val, epochs=5, patience=2):\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "    return history.history['val_accuracy'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be60e2ea",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "be60e2ea"
      },
      "source": [
        "# Neural Architecture Search Function\n",
        "\n",
        "This function performs a comprehensive neural architecture search with knowledge distillation, pruning, and quantization.\n",
        "\n",
        "1. Search Space Definition:\n",
        "   - Defines the range of architectural choices (e.g., number of convolutional blocks, layers per block, initial filters, dense layers).\n",
        "\n",
        "2. Data Splitting:\n",
        "   - Splits the training data to create a smaller subset for quick evaluation.\n",
        "\n",
        "3. Architecture Generation:\n",
        "   - Iterates through different combinations of architectural parameters to generate various model configurations.\n",
        "\n",
        "4. Quick Evaluation:\n",
        "   - For each generated architecture, performs a rapid evaluation using a subset of the data.\n",
        "   - This step helps to quickly filter out poorly performing architectures without spending time on full training.\n",
        "\n",
        "5. Knowledge Distillation:\n",
        "   - For promising architectures (those passing the quick evaluation threshold), applies knowledge distillation.\n",
        "   - Uses a pre-trained teacher model to guide the training of the student (generated) model.\n",
        "   - Combines the standard cross-entropy loss with a distillation loss that encourages the student to mimic the teacher's softened outputs.\n",
        "\n",
        "6. Pruning:\n",
        "   - Applies network pruning to reduce model size and potentially improve generalization.\n",
        "   - Uses polynomial decay pruning schedule, gradually increasing sparsity over time.\n",
        "\n",
        "7. Quantization:\n",
        "   - Applies quantization to further reduce model size and improve inference speed.\n",
        "   - Converts the model to TensorFlow Lite format with default optimizations.\n",
        "\n",
        "8. Final Evaluation:\n",
        "   - Assesses the pruned and quantized model on the test set to get the final accuracy.\n",
        "\n",
        "9. Model Tracking:\n",
        "   - Keeps track of the top 5 models based on accuracy and model size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a7805b",
      "metadata": {
        "id": "b6a7805b"
      },
      "outputs": [],
      "source": [
        "def search_architectures(x_train, y_train, x_test, y_test, input_shape, num_classes, teacher):\n",
        "    best_models = deque(maxlen=5)\n",
        "    best_metrics = (float('inf'), float('inf'), 0)  # (size, params, accuracy)\n",
        "\n",
        "    # Define the search space\n",
        "    conv_blocks = [1, 2, 3]  # Number of convolutional blocks\n",
        "    conv_layers_per_block = [1, 2]  # Conv layers in each block\n",
        "    initial_filters = [16, 32, 64]  # Initial number of filters\n",
        "    dense_layers = [0, 1, 2]  # Number of dense layers before the final layer\n",
        "    dense_units_options = [64, 128, 256, 512]  # Units in dense layers\n",
        "\n",
        "    # Split the training data for quick evaluation\n",
        "    x_train_quick, x_val_quick, y_train_quick, y_val_quick = train_test_split(\n",
        "        x_train, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    for n_blocks in conv_blocks:\n",
        "        for layers_per_block in conv_layers_per_block:\n",
        "            for init_filters in initial_filters:\n",
        "                for n_dense in dense_layers:\n",
        "                    for dense_units in dense_units_options:\n",
        "                        # Build the model configuration\n",
        "                        config = []\n",
        "                        filters = init_filters\n",
        "                        for _ in range(n_blocks):\n",
        "                            for _ in range(layers_per_block):\n",
        "                                config.append(('Conv2D', filters))\n",
        "                            config.append(('MaxPool2D', None))\n",
        "                            filters *= 2  # Double the number of filters\n",
        "\n",
        "                        config.append(('GlobalAveragePooling2D', None))\n",
        "\n",
        "                        for _ in range(n_dense):\n",
        "                            config.append(('Dense', dense_units))\n",
        "\n",
        "                        # Final classification layer\n",
        "                        config.append(('Dense', num_classes))\n",
        "\n",
        "                        try:\n",
        "                            student = create_model(config, input_shape, num_classes)\n",
        "                            student.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                        # Quick evaluation\n",
        "                        quick_accuracy = quick_evaluate(student, x_train_quick, y_train_quick, x_val_quick, y_val_quick)\n",
        "\n",
        "                        # Only proceed with full training if the model shows promise\n",
        "                        if quick_accuracy > 0.6:\n",
        "                            # Apply knowledge distillation\n",
        "                            distiller = Distiller(student=student, teacher=teacher)\n",
        "                            distiller.compile(\n",
        "                                optimizer=keras.optimizers.Adam(),\n",
        "                                metrics=[keras.metrics.CategoricalAccuracy()],\n",
        "                                student_loss_fn=keras.losses.CategoricalCrossentropy(),\n",
        "                                distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "                                alpha=0.1,\n",
        "                                temperature=3,\n",
        "                            )\n",
        "                            distiller.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=0)\n",
        "\n",
        "                            # Apply pruning\n",
        "                            pruning_params = {\n",
        "                                'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "                                    initial_sparsity=0.50,\n",
        "                                    final_sparsity=0.80,\n",
        "                                    begin_step=0,\n",
        "                                    end_step=len(x_train) * 10\n",
        "                                )\n",
        "                            }\n",
        "                            pruned_model = tfmot.sparsity.keras.prune_low_magnitude(distiller.student, **pruning_params)\n",
        "                            pruned_model.compile(\n",
        "                                optimizer=keras.optimizers.Adam(),\n",
        "                                loss=keras.losses.CategoricalCrossentropy(),\n",
        "                                metrics=[keras.metrics.CategoricalAccuracy()],\n",
        "                            )\n",
        "\n",
        "                            with tf.device('/cpu:0'):\n",
        "                                pruned_model.fit(x_train, y_train, epochs=1, validation_split=0.2, callbacks=[\n",
        "                                    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "                                    tfmot.sparsity.keras.PruningSummaries(log_dir=tempfile.mkdtemp()),\n",
        "                                ], verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "                            # Apply quantization-aware training\n",
        "                            quantize_model = tfmot.quantization.keras.quantize_model\n",
        "                            stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "                            q_aware_model = quantize_model(stripped_pruned_model)\n",
        "\n",
        "                            q_aware_model.compile(\n",
        "                                optimizer=keras.optimizers.Adam(),\n",
        "                                loss=keras.losses.CategoricalCrossentropy(),\n",
        "                                metrics=[keras.metrics.CategoricalAccuracy()],\n",
        "                            )\n",
        "\n",
        "                            q_aware_model.fit(x_train, y_train, epochs=5, validation_split=0.2, verbose=0)\n",
        "\n",
        "                            # Evaluate the quantization-aware model\n",
        "                            _, accuracy = q_aware_model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "                            # Convert to TFLite\n",
        "                            converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "                            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "                            quantized_tflite_model = converter.convert()\n",
        "\n",
        "                            # Save and get model size\n",
        "                            _, tflite_file = tempfile.mkstemp('.tflite')\n",
        "                            with open(tflite_file, 'wb') as f:\n",
        "                                f.write(quantized_tflite_model)\n",
        "\n",
        "                            model_size = os.path.getsize(tflite_file) / float(2**20)  # Size in MB\n",
        "                            num_params = q_aware_model.count_params()\n",
        "\n",
        "                            print(f\"Configuration: {config}\")\n",
        "                            print(f\"Quick Accuracy: {quick_accuracy:.4f}\")\n",
        "                            print(f\"Final Accuracy: {accuracy:.4f}\")\n",
        "                            print(f\"Model size: {model_size:.2f} MB\")\n",
        "                            print(f\"Number of parameters: {num_params}\")\n",
        "                            print(\"--------------------\")\n",
        "\n",
        "                            best_models.append((q_aware_model, accuracy, model_size, num_params, config))\n",
        "                            best_models = deque(sorted(best_models, key=lambda x: (x[1], -x[2]), reverse=True)[:5])\n",
        "\n",
        "                        else:\n",
        "                            print(f\"Configuration: {config}\")\n",
        "                            print(f\"Quick Accuracy: {quick_accuracy:.4f} - Skipped\")\n",
        "                            print(\"--------------------\")\n",
        "\n",
        "    return best_models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021016bd",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "021016bd"
      },
      "source": [
        "# Data Preparation and Teacher Model Creation\n",
        "\n",
        "This section loads and preprocesses the CIFAR-10 dataset, converting it to grayscale. It also defines and trains the teacher model that will be used for knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9386ef4",
      "metadata": {
        "scrolled": true,
        "id": "c9386ef4"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "\n",
        "\n",
        "# Convert to grayscale\n",
        "x_train = np.dot(x_train[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "x_test = np.dot(x_test[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 32, 32, 1))\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 32, 32, 1))\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "\n",
        "# Create the teacher model\n",
        "teacher = keras.Sequential([\n",
        "    keras.Input(shape=(32, 32, 1)),\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D((2, 2)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D((2, 2)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D((2, 2)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform'),\n",
        "    keras.layers.Dense(256, activation='relu', kernel_initializer='he_uniform'),\n",
        "    keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile and train the teacher\n",
        "teacher.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "teacher.fit(x_train, y_train, epochs=20, validation_split=0.2, verbose=1)\n",
        "\n",
        "def curriculum_sort(x_train, y_train, teacher):\n",
        "    # sort based on the entropy of the teacher's predictions\n",
        "    teacher_preds = teacher.predict(x_train)\n",
        "    entropies = -np.sum(teacher_preds * np.log(teacher_preds + 1e-10), axis=1)\n",
        "    sorted_indices = np.argsort(entropies)\n",
        "    return x_train[sorted_indices], y_train[sorted_indices]\n",
        "\n",
        "x_train, y_train = curriculum_sort(x_train, y_train, teacher)\n",
        "\n",
        "\n",
        "# Run the architecture search\n",
        "best_model, (best_size, best_params, best_accuracy) = search_architectures(\n",
        "    x_train, y_train, x_test, y_test, input_shape=(32, 32, 1), num_classes=10, teacher=teacher\n",
        ")\n",
        "\n",
        "print(\"Best model found:\")\n",
        "print(f\"Size: {best_size:.2f} MB\")\n",
        "print(f\"Parameters: {best_params}\")\n",
        "print(f\"Accuracy: {best_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e33965c8",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "e33965c8"
      },
      "source": [
        "# Visualisation Function\n",
        "\n",
        "This function creates a scatter plot of the top models, comparing their accuracy, number of parameters, and model size. It also prints detailed information about each top model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ee8c96",
      "metadata": {
        "id": "a0ee8c96"
      },
      "outputs": [],
      "source": [
        "def plot_top_models(top_models):\n",
        "    accuracies = [model[1] for model in top_models]\n",
        "    params = [model[3] for model in top_models]\n",
        "    sizes = [model[2] for model in top_models]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(params, accuracies, c=sizes, s=100, cmap='viridis')\n",
        "    plt.colorbar(scatter, label='Model Size (MB)')\n",
        "\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Number of Parameters')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Top 5 Models: Accuracy vs Number of Parameters')\n",
        "\n",
        "    for i, model in enumerate(top_models):\n",
        "        plt.annotate(f\"Model {i+1}\", (params[i], accuracies[i]), xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed information about each model\n",
        "    for i, (model, accuracy, size, num_params, config) in enumerate(top_models, 1):\n",
        "        print(f\"Model {i}:\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  Size: {size:.2f} MB\")\n",
        "        print(f\"  Parameters: {num_params}\")\n",
        "        print(f\"  Configuration: {config}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "# Run the architecture search\n",
        "# top_models = search_architectures(x_train, y_train, x_test, y_test, input_shape=(32, 32, 1), num_classes=10, teacher=teacher)\n",
        "\n",
        "# Plot the results\n",
        "plot_top_models(best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d294cc64",
      "metadata": {
        "id": "d294cc64"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}